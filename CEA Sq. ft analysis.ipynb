{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "\n",
    "# CADES configuration\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# cluster = SLURMCluster(project='birthright', queue='high_mem_cd', cores=30, memory='150 GB', job_extra=[\"-N 1\"], interface=\"ib0\")\n",
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate the square footage data table from the raw npy files.\n",
    "# def make_sqft_dict():\n",
    "#     dict_sqft = {\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_1.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_2.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_3.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_4.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_5.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_6.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_7.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_8.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_9.npy', allow_pickle=True).flatten()))[0],\n",
    "#         **dict(enumerate(np.load('data/reference/sq_ft/dict_10.npy', allow_pickle=True).flatten()))[0]\n",
    "#     }\n",
    "#     return(dict_sqft)\n",
    "# dict_sqft = make_sqft_dict()\n",
    "# # Do this nonsense with the indexes to get them persisted to parquet\n",
    "# df = pd.DataFrame.from_dict(dict_sqft, orient='index', columns=['sqft36']).reset_index().set_index('index')\n",
    "# # df.head()\n",
    "# df.to_parquet(\"data/reference/sq_ft/sg_sqft.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(d):\n",
    "    if \":\" == d[-3:-2]:\n",
    "        d = d[:-3]+d[-2:]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = dd.read_csv(\"data/safegraph/weekly-patterns/2020-04*.csv\", dtype={'naics_code': 'float64'}).set_index('safegraph_place_id')\n",
    "patterns['date'] = patterns['date_range_start'].apply(lambda x: datetime.strptime(to_date(x), '%Y-%m-%dT%H:%M:%S%z').date(), meta=('date_range_start', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_ft = dd.read_parquet(\"data/reference/sq_ft/sg_sqft.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do an inner join to focus on the POIs that we actually have data for.\n",
    "df = dd.merge(patterns, sq_ft)\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={'index': 'safegraph_place_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_int(lists):\n",
    "    ints = []\n",
    "    for x in lists:\n",
    "        try:\n",
    "            ints.append(int(x))\n",
    "        except:\n",
    "            ints.append(0)\n",
    "    return np.array(ints).astype(np.int32)\n",
    "\n",
    "def get_cbgs(df):   \n",
    "    df = df[['safegraph_place_id', 'date','visitor_home_cbgs', 'visits_by_day']]\n",
    "    df = df.visitor_home_cbgs\\\n",
    "        .str.translate(str\\\n",
    "        .maketrans({'{':'', '}':'','\"':''}))\\\n",
    "        .str.split(',')\\\n",
    "        .apply(pd.Series, 1)\\\n",
    "        .stack() \\\n",
    "        .reset_index(level=1, drop=True) \\\n",
    "        .to_frame('cbgs') \\\n",
    "        .merge(df, left_index=True, right_index=True)\\\n",
    "        .drop(['visitor_home_cbgs'], axis=1) \\\n",
    "        .rename(columns={0:'cbgs',1:'party'}) \\\n",
    "        .dropna()\n",
    "    \n",
    "    df = df[df.cbgs != \"\"]\n",
    "    df['party'] = df.cbgs.str.split(':').apply(lambda x: x[-1]).astype(np.int32)\n",
    "    df['cbgs'] = df.cbgs.str.split(':').apply(lambda x: x[0]).astype('str')\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df_blocks = df.map_partitions(get_cbgs, meta={'cbgs': 'str', 'safegraph_place_id': 'str', 'date': 'object', 'visits_by_day': 'object', 'party': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_firms = df[['safegraph_place_id','location_name','brands','naics_code']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_contacts(df):\n",
    "    df = df[['safegraph_place_id', 'visits_by_each_hour','median_dwell','date','sqft36']].copy()\n",
    "    # Generate list of visits by hour\n",
    "    df['close_contacts'] = None\n",
    "    # Turn string into a list of strings\n",
    "    df.close_contacts = df.visits_by_each_hour.str.translate(str.maketrans({'[':'', ']':'','\"':''})).str.split(',')\n",
    "    # Turn list of strings into a list of ints with each int indicating visitors during each hour of the week\n",
    "    df.close_contacts = df.apply(lambda row: [int(i) for i in row.close_contacts], axis=1)\n",
    "    # Turn the list of ints into a chunks of 24 hours for each day and calculate the average interactions per 36 square feet\n",
    "    df.close_contacts = df.apply(lambda row: [np.divide(\\\n",
    "                                              pd.DataFrame(c).rolling(int(max(row.median_dwell/60, 1)), min_periods=1).sum()[0].tolist()\\\n",
    "                                              , row.sqft36)\\\n",
    "                                              .mean()\n",
    "                                              for c in chunks(row.close_contacts, 24)], axis=1)\n",
    "    df = df.drop(['visits_by_each_hour','median_dwell','sqft36'], axis = 1)\n",
    "    df = df.close_contacts.apply(pd.Series)\\\n",
    "         .merge(df, left_index=True, right_index=True)\\\n",
    "         .drop(['close_contacts'], axis=1)\\\n",
    "         .melt(id_vars=['safegraph_place_id','date'],value_name='visits')\n",
    "    df['d2'] = (df.date + pd.to_timedelta(pd.np.ceil(df.variable.astype(float)), unit=\"D\"))\n",
    "    df = df.drop(['variable'], axis = 1)\n",
    "    print(df)\n",
    "    return (df)\n",
    "\n",
    "df_contacts = df.map_partitions(get_contacts, meta={'safegraph_place_id': 'str', 'date': 'object', 'visits': 'int32', 'd2': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_contacts.merge(df_blocks, on=['safegraph_place_id','date'])\n",
    "df = df.merge(df_firms, left_on='safegraph_place_id', right_on='safegraph_place_id') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
